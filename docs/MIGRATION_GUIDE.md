# üîÑ Gu√≠a de Migraci√≥n: BigQuery ‚Üí Supabase

Esta gu√≠a te ayudar√° a migrar los datos de TINSA desde Google BigQuery a tu base de datos Supabase.

## üìã Requisitos Previos

1. **Acceso a Google Cloud Console**
   - Proyecto: `my-project-wap-486916`
   - Dataset: `BBDDTINSATables`
   - Tabla: `BBDDTINSA_PYTO_CENTROcsv_1770655119181`

2. **Credenciales de Google Cloud**
   - Service Account con permisos de BigQuery

## üöÄ Pasos de Configuraci√≥n

### 1. Instalar Dependencias

```bash
cd backend
.venv/bin/pip install -r requirements-bigquery.txt
```

Esto instalar√°:
- `google-cloud-bigquery` - Cliente de BigQuery
- `pandas` - Procesamiento de datos
- `db-dtypes` - Tipos de datos de BigQuery

### 2. Configurar Credenciales de Google Cloud

#### Opci√≥n A: Descargar desde Google Cloud Console (Recomendado)

1. Ve a: https://console.cloud.google.com/iam-admin/serviceaccounts?project=my-project-wap-486916

2. Crea o selecciona una Service Account

3. Genera una clave JSON:
   - Click en ‚ãÆ (men√∫) ‚Üí "Manage keys"
   - "Add Key" ‚Üí "Create new key" ‚Üí JSON
   - Descarga el archivo

4. Guarda el archivo como:
   ```
   backend/credentials/gcp-key.json
   ```

#### Opci√≥n B: Usar gcloud CLI

```bash
gcloud auth application-default login
```

### 3. Verificar Configuraci√≥n

```bash
cd backend
.venv/bin/python -m app.etl.bigquery_to_supabase --preview
```

Este comando:
- ‚úÖ Verifica las credenciales
- ‚úÖ Muestra el esquema de la tabla
- ‚úÖ Muestra 20 registros de ejemplo
- ‚úÖ Lista todas las columnas disponibles

## üîç Exploraci√≥n de Datos

### Ver Esquema de la Tabla

```bash
.venv/bin/python -m app.etl.bigquery_to_supabase --preview
```

Esto mostrar√°:
- Nombres de todas las columnas
- Tipos de datos
- Total de filas en BigQuery

### Previsualizar Datos (Dry Run)

```bash
.venv/bin/python -m app.etl.bigquery_to_supabase
```

Modo dry-run (por defecto):
- ‚úÖ Extrae datos de BigQuery
- ‚úÖ Transforma seg√∫n el mapeo
- ‚ùå NO inserta en Supabase
- ‚úÖ Muestra estad√≠sticas

## üéØ Mapeo de Campos

**IMPORTANTE**: Antes de ejecutar la migraci√≥n, debes ajustar el mapeo de campos en:
`backend/app/etl/bigquery_to_supabase.py`

### Funci√≥n a Personalizar: `map_tinsa_to_supabase()`

```python
def map_tinsa_to_supabase(df: pd.DataFrame) -> list:
    # Ajusta estos campos seg√∫n los nombres reales en BigQuery
    for _, row in df.iterrows():
        project = {
            "name": row.get("NOMBRE_CAMPO_REAL", "Sin nombre"),
            "developer": row.get("INMOBILIARIA_CAMPO_REAL", None),
            "commune": row.get("COMUNA_CAMPO_REAL", None),
            # ... m√°s campos
        }
```

### Campos Disponibles en Supabase

Seg√∫n tu esquema (`20260209000000_initial_schema.sql`):

**Campos Obligatorios:**
- `name` - Nombre del proyecto
- `commune` - Comuna
- `region` - Regi√≥n

**Campos Opcionales:**
- `developer` - Inmobiliaria
- `address` - Direcci√≥n
- `latitude`, `longitude` - Coordenadas
- `total_units`, `sold_units`, `available_units` - Unidades
- `avg_price_uf`, `avg_price_m2_uf` - Precios
- `sales_speed_monthly` - Velocidad de ventas
- `months_to_sell_out` - MAO
- `project_status` - Estado
- `property_type` - Tipo de propiedad
- `category` - Categor√≠a
- `delivery_date` - Fecha de entrega

## üîÑ Ejecutar Migraci√≥n

### Paso 1: Previsualizar (Dry Run)

```bash
.venv/bin/python -m app.etl.bigquery_to_supabase
```

Revisa:
- ‚úÖ Cantidad de registros
- ‚úÖ Mapeo de campos
- ‚úÖ Datos transformados

### Paso 2: Ejecutar Migraci√≥n Real

```bash
.venv/bin/python -m app.etl.bigquery_to_supabase --migrate
```

Esto:
- ‚úÖ Extrae TODOS los datos de BigQuery
- ‚úÖ Transforma seg√∫n el mapeo
- ‚úÖ Inserta en Supabase en batches de 100
- ‚úÖ Usa `upsert` para evitar duplicados (por `name` + `commune`)

## üìä Monitoreo

Durante la migraci√≥n ver√°s:

```
üöÄ Iniciando migraci√≥n BigQuery ‚Üí Supabase

üìã Esquema de la tabla BigQuery:
  - nombre_proyecto: STRING
  - comuna: STRING
  - precio_uf: FLOAT
  ...

‚úÖ Total de filas: 1,234

üîÑ Iniciando migraci√≥n de 1,234 registros...
‚úÖ Datos extra√≠dos: 1,234 registros
‚úÖ Datos transformados: 1,234 proyectos
  ‚úÖ Insertados 100/1,234 proyectos...
  ‚úÖ Insertados 200/1,234 proyectos...
  ...
üéâ Migraci√≥n completada: 1,234 proyectos insertados
```

## ‚ö†Ô∏è Consideraciones

### Duplicados
- El script usa `upsert` con clave √∫nica: `(name, commune)`
- Si un proyecto ya existe, se actualizar√°
- Si es nuevo, se insertar√°

### Errores Comunes

1. **"GOOGLE_APPLICATION_CREDENTIALS no est√° configurado"**
   - Soluci√≥n: Descarga y configura el archivo JSON de credenciales

2. **"Archivo de credenciales no encontrado"**
   - Soluci√≥n: Verifica que `backend/credentials/gcp-key.json` existe

3. **"Permission denied"**
   - Soluci√≥n: La Service Account necesita rol `BigQuery Data Viewer`

4. **"Column not found"**
   - Soluci√≥n: Ajusta el mapeo en `map_tinsa_to_supabase()` con los nombres reales

## üîß Personalizaci√≥n Avanzada

### Cambiar Tama√±o de Batch

En `bigquery_to_supabase.py`:

```python
BATCH_SIZE = 100  # Cambiar a 50, 200, etc.
```

### Filtrar Datos

Modifica la query en `migrate_data()`:

```python
query = f"""
SELECT * 
FROM `{PROJECT_ID}.{DATASET_ID}.{TABLE_ID}`
WHERE comuna IN ('Las Condes', 'Providencia')
AND precio_uf > 5000
"""
```

### Transformaciones Personalizadas

En `map_tinsa_to_supabase()`:

```python
# Ejemplo: Normalizar nombres de comunas
commune = row.get("comuna", "").strip().title()

# Ejemplo: Calcular campos derivados
available = total - sold

# Ejemplo: Convertir fechas
delivery_date = pd.to_datetime(row.get("fecha_entrega")).date()
```

## üìù Siguiente Paso

Una vez completada la migraci√≥n, verifica los datos:

```bash
# En Supabase SQL Editor
SELECT COUNT(*) FROM projects;
SELECT * FROM projects LIMIT 10;
```

O visita el dashboard:
- http://localhost:3000/dashboard/projects
- http://localhost:3000/dashboard/map

## üÜò Soporte

Si encuentras problemas:
1. Ejecuta con `--preview` para ver los datos sin migrar
2. Revisa los nombres de columnas en BigQuery
3. Ajusta el mapeo en `map_tinsa_to_supabase()`
4. Ejecuta en dry-run antes de la migraci√≥n real
